{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Download data and unzip"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-04-11T17:18:07.291742Z","iopub.status.busy":"2022-04-11T17:18:07.289504Z","iopub.status.idle":"2022-04-11T17:19:11.541084Z","shell.execute_reply":"2022-04-11T17:19:11.540155Z","shell.execute_reply.started":"2022-04-11T17:18:07.291699Z"},"trusted":true},"outputs":[],"source":["!rm -rf *\n","!wget https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n","!unzip -q nature_12K.zip\n","!rm nature_12K.zip \n","!find . -name '.DS_Store' -type f -delete\n","!pip install rich wandb"]},{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:19:38.491474Z","iopub.status.busy":"2022-04-11T17:19:38.490966Z","iopub.status.idle":"2022-04-11T17:19:45.145907Z","shell.execute_reply":"2022-04-11T17:19:45.145044Z","shell.execute_reply.started":"2022-04-11T17:19:38.491434Z"},"trusted":true},"outputs":[],"source":["import yaml\n","import os\n","import cv2\n","import time\n","import glob\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","from PIL import Image\n","from rich import print\n","from pprint import pprint\n","from cv2 import imread, cvtColor\n","\n","from sklearn.utils import shuffle\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import  Dense, Input, InputLayer, Flatten, Conv2D, BatchNormalization, MaxPooling2D, Activation , GlobalAveragePooling2D\n","\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2 as IRV2\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.applications.xception import Xception\n","\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","import wandb\n","from wandb.keras import WandbCallback"]},{"cell_type":"markdown","metadata":{},"source":["# Constants"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:19:48.325788Z","iopub.status.busy":"2022-04-11T17:19:48.325030Z","iopub.status.idle":"2022-04-11T17:19:48.334426Z","shell.execute_reply":"2022-04-11T17:19:48.333142Z","shell.execute_reply.started":"2022-04-11T17:19:48.325748Z"},"trusted":true},"outputs":[],"source":["TRAIN_DIR = \"./inaturalist_12K/train/\"\n","TEST_DIR = \"./inaturalist_12K/val/\"\n","os.environ[\"WANDB_API_KEY\"] = \"f058e3418fb166c3bee2d5131ef0bcc4b642793f\"\n","IMAGE_SIZE = (224,224)\n","\n","class_labels = ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n","class_labels = sorted(class_labels)"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:19:49.911291Z","iopub.status.busy":"2022-04-11T17:19:49.910756Z","iopub.status.idle":"2022-04-11T17:19:49.935346Z","shell.execute_reply":"2022-04-11T17:19:49.934466Z","shell.execute_reply.started":"2022-04-11T17:19:49.911254Z"},"trusted":true},"outputs":[],"source":["class Model():\n","\n","    def __init__(self, image_size, config):\n","        \n","        self.IMG_HEIGHT = image_size[0]\n","        self.IMG_WIDTH = image_size[1]        \n","        self.input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)\n","        \n","        self.num_hidden_cnn_layers= config[\"num_hidden_cnn_layers\"]\n","        self.filter_size = config[\"filter_size\"]\n","        \n","        self.number_of_filters_base  = config[\"number_of_filters_base\"]\n","        self.filter_distribution = config[\"filter_distribution\"]\n","        \n","        self.padding = config[\"padding\"]\n","        self.activation = config[\"activation\"]\n","        self.optimizer = config[\"optimizer\"]\n","        \n","        self.pool_size = config[\"pool_size\"]\n","\n","        self.batch_normalization = config[\"batch_normalization\"]\n","        self.batch_normalisation_location = config[\"batch_normalisation_location\"]  \n","        self.dropout_fraction = config[\"dropout_fraction\"]\n","        self.dropout_location = config[\"dropout_location\"]\n","        \n","        self.global_average_pooling = config[\"global_average_pooling\"]\n","        self.dense_neurons = config[\"dense_neurons\"]\n","        self.num_classes = config[\"num_classes\"]\n","\n","\n","    def build_cnnmodel(self):\n","        with tf.device('/device:GPU:0'):\n","            tf.keras.backend.clear_session()\n","\n","            model = Sequential()\n","            model.add(\n","                Conv2D(\n","                    self.number_of_filters_base, \n","                    self.filter_size, \n","                    padding = self.padding,\n","                    kernel_initializer = \"he_uniform\",\n","                    input_shape = (self.IMG_HEIGHT, self.IMG_WIDTH, 3)\n","                    )\n","            )\n","            \n","            # if batchnorm is True, location is Before, add batch normalisation layer then activation\n","            if self.batch_normalisation_location == \"Before\" and self.batch_normalization: \n","                model.add(BatchNormalization())\n","            \n","            model.add(Activation(self.activation))\n","            \n","            # if batchnorm is True, location is After, add activation then batch normalisation layer\n","            if self.batch_normalisation_location == \"After\" and self.batch_normalization: model.add(BatchNormalization())\n","            \n","            # Max pooling\n","            model.add(MaxPooling2D(pool_size=self.pool_size))  \n","            \n","            # Dropout\n","            if self.dropout_fraction != None:\n","                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n","\n","            # Convolutional layers\n","            for i in range(self.num_hidden_cnn_layers-1):\n","                # TODO: changing the kernel size in each layer\n","                # filter distribution \n","                # double - double number of filters in each Convolutional layers\n","                # half - halve the filter size in each successive convolutional layers\n","                # o/w same number of filters in each Convolutional layers\n","\n","                if self.filter_distribution == \"double\":\n","                    model.add(\n","                        Conv2D(\n","                            2**(i+1)*self.number_of_filters_base, \n","                            self.filter_size,\n","                            kernel_initializer = \"he_uniform\", \n","                            padding = self.padding\n","                        )\n","                    )\n","                elif self.filter_distribution == \"half\":\n","                    model.add(\n","                        Conv2D(\n","                            int(self.number_of_filters_base/2**(i+1)), \n","                            self.filter_size,\n","                            kernel_initializer = \"he_uniform\", \n","                            padding = self.padding\n","                        )\n","                    )\n","                else:\n","                    model.add(\n","                        Conv2D(\n","                            self.number_of_filters_base,\n","                            self.filter_size,\n","                            kernel_initializer=\"he_uniform\",\n","                            padding=self.padding\n","                        )\n","                    )\n","\n","                if self.batch_normalisation_location == \"Before\" and self.batch_normalization: \n","                    model.add(BatchNormalization())\n","                \n","                model.add(Activation(self.activation))\n","            \n","                if self.batch_normalisation_location == \"After\" and self.batch_normalization: \n","                    model.add(BatchNormalization())\n","                \n","                # Max pooling\n","                model.add(MaxPooling2D(pool_size=self.pool_size))\n","                \n","                # Dropout\n","                if self.dropout_fraction != None and (self.dropout_location == \"convlayer\" or self.dropout_location == \"all\"):\n","                    model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n","\n","            # Global average pooling    \n","            if self.global_average_pooling == True:\n","                model.add(GlobalAveragePooling2D())\n","            else:\n","                model.add(Flatten())\n","\n","            model.add(Dense(self.dense_neurons, activation = 'sigmoid'))\n","\n","            if self.dropout_fraction != None and (self.dropout_location == \"denselayer\" or self.dropout_location == \"all\"):\n","                model.add(tf.keras.layers.Dropout(self.dropout_fraction))\n","            \n","            model.add(Dense(self.num_classes, activation = 'softmax'))\n","\n","            return model"]},{"cell_type":"markdown","metadata":{},"source":["# Data Generator"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:19:51.817742Z","iopub.status.busy":"2022-04-11T17:19:51.817334Z","iopub.status.idle":"2022-04-11T17:19:51.827142Z","shell.execute_reply":"2022-04-11T17:19:51.825349Z","shell.execute_reply.started":"2022-04-11T17:19:51.817700Z"},"trusted":true},"outputs":[],"source":["def get_data_generator(train_dir, test_dir,  data_augmentation, batch_size, img_size):\n","    \n","    if data_augmentation == True:\n","        train_datagen = ImageDataGenerator(\n","            rescale=1./255,\n","            validation_split = 0.1,\n","            zoom_range=0.2,\n","            rotation_range=10, \n","            horizontal_flip=True,\n","        )\n","    \n","    else:\n","        train_datagen = ImageDataGenerator(rescale=1./255, validation_split = 0.1)\n","\n","    test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","    train_generator = train_datagen.flow_from_directory(\n","        train_dir,\n","        subset='training',\n","        target_size=img_size,\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle = True,\n","        seed = 42\n","    )\n","        \n","    validation_generator = train_datagen.flow_from_directory(\n","        train_dir,\n","        subset = 'validation',\n","        target_size=img_size,\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle = False,\n","        seed = 42\n","    )\n","\n","    test_generator = test_datagen.flow_from_directory(\n","        test_dir,\n","        target_size=img_size,\n","        batch_size=batch_size,\n","        class_mode='categorical',\n","        shuffle = False,\n","        seed = 42\n","    )\n","    \n","    return train_generator, validation_generator, test_generator"]},{"cell_type":"markdown","metadata":{},"source":["# Train and Test function"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:19:53.622644Z","iopub.status.busy":"2022-04-11T17:19:53.622354Z","iopub.status.idle":"2022-04-11T17:19:53.634487Z","shell.execute_reply":"2022-04-11T17:19:53.633774Z","shell.execute_reply.started":"2022-04-11T17:19:53.622615Z"},"trusted":true},"outputs":[],"source":["def train():\n","    \n","    config_defaults = dict(\n","        image_size = IMAGE_SIZE,\n","\n","        num_hidden_cnn_layers = 5,\n","        filter_size = (3,3),\n","\n","        number_of_filters_base  = 128,\n","        filter_distribution = \"half\",\n","\n","        padding = 'valid',\n","        activation = 'relu',\n","        optimizer = 'nadam',\n","        \n","        pool_size = (2,2),\n","        \n","        batch_normalization = True,\n","        batch_normalisation_location = \"After\",\n","        dropout_fraction = 0.01,\n","        dropout_location = \"dense\",\n","        \n","        global_average_pooling = True,\n","        dense_neurons = 256,\n","        num_classes = 10,\n","        \n","        epochs = 5,\n","        batch_size = 64,\n","        data_augmentation = False,\n","    ) \n","\n","\n","    # WandB logging \n","    wandb.init(project = 'CS6910_Assignment2', config = config_defaults)\n","    CONFIG = wandb.config\n","    # Run name  \n","    wandb.run.name = \"CNN_\" + str(CONFIG.num_hidden_cnn_layers) + \"_dn_\" + str(CONFIG.dense_neurons) \\\n","    + \"_opt_\" + CONFIG.optimizer + \"_dro_\" + str(CONFIG.dropout_fraction) + \"_bs_\"+str(CONFIG.batch_size) \\\n","    + \"_fd_\" + CONFIG.filter_distribution + \"_bnl_\" + CONFIG.batch_normalisation_location + \"_dpl_\" + CONFIG.dropout_location\n","\n","\n","    data_augmentation = CONFIG.data_augmentation\n","    BATCH_SIZE = CONFIG.batch_size\n","    train_generator, validation_generator, test_generator = get_data_generator(TRAIN_DIR, TEST_DIR,  data_augmentation, BATCH_SIZE, CONFIG.image_size)\n","\n","    with tf.device('/device:GPU:0'):        \n","        _model = Model(CONFIG.image_size, CONFIG)\n","        model = _model.build_cnnmodel()\n","\n","        model.compile(\n","            optimizer=CONFIG.optimizer,  \n","            loss=tf.keras.losses.CategoricalCrossentropy(),\n","            metrics=['accuracy'],\n","        )\n","      \n","        history = model.fit(\n","            train_generator,\n","            steps_per_epoch = train_generator.samples // CONFIG.batch_size,\n","            validation_data = validation_generator, \n","            validation_steps = validation_generator.samples // CONFIG.batch_size,\n","            epochs = CONFIG.epochs, \n","            callbacks=[WandbCallback()]\n","        )\n","\n","        model.save('./TrainedModel/'+wandb.run.name)\n","        test_loss, test_acc = model.evaluate(test_generator)\n","        \n","        # Log the test loss and accuracy\n","        wandb.log(\n","            {\n","                \"test_acc\": test_acc,\n","                \"test_loss\": test_loss, \n","            }\n","        )\n","        wandb.finish()\n","        \n","        return model, history"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:19:56.371239Z","iopub.status.busy":"2022-04-11T17:19:56.370441Z","iopub.status.idle":"2022-04-11T17:32:29.558621Z","shell.execute_reply":"2022-04-11T17:32:29.557935Z","shell.execute_reply.started":"2022-04-11T17:19:56.371188Z"},"trusted":true},"outputs":[],"source":["model, history = train()"]},{"cell_type":"markdown","metadata":{},"source":["# Sweep Config"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:35:00.879899Z","iopub.status.busy":"2022-04-11T17:35:00.879571Z","iopub.status.idle":"2022-04-11T17:35:01.880946Z","shell.execute_reply":"2022-04-11T17:35:01.880054Z","shell.execute_reply.started":"2022-04-11T17:35:00.879852Z"},"trusted":true},"outputs":[],"source":["sweep_config = {\n","    \"name\": \"Test Sweep\",\n","    \"method\": \"bayes\",\n","    \"metric\":{\n","        \"name\": \"val_accuracy\",\n","        \"goal\": \"maximize\"\n","    },\n","    'early_terminate': {\n","        'type':'hyperband',\n","        'min_iter': 3,\n","        's': 2\n","    },\n","    \"parameters\": {\n","        \"epochs\": {\n","            \"values\": [5, 10]\n","        },\n","        \"data_augmentation\": {\n","            \"values\": [True, False]\n","        },\n","        \"batch_size\": {\n","            \"values\": [32, 64]\n","        },\n","        \"filter_size\": {\n","            \"values\": [(7,7), (5,5), (3,3)]\n","        },\n","        \"number_of_filters_base\": {\n","            \"values\": [32, 64]\n","        },\n","        \"filter_distribution\": {\n","            \"values\": [\"half\", \"full\", \"same\"]\n","        },\n","        \"activation\":{\n","            \"values\": [\"relu\", \"elu\", \"selu\"]\n","        },\n","        \"padding\": {\n","            \"values\": [\"valid\", \"same\"]\n","        },\n","        \"optimizer\": {\n","            \"values\": [\"adam\", \"nadam\"]\n","        },\n","        \"batch_normalization\": {\n","            \"values\": [True, False]\n","        },\n","        \"batch_normalisation_location\": {\n","            \"values\": [\"Before\", \"After\"]\n","        },\n","        \"dropout_fraction\": {\n","            \"values\": [None, 0.2,0.3]\n","        },  \n","        \"dropout_location\": {\n","            \"values\": [\"convlayer\", \"denselayer\", \"all\"]\n","        },\n","        \"dense_neurons\": {\n","            \"values\": [32, 64, 128]\n","        },   \n","        \"global_average_pooling\": {\n","            \"values\": [False,True]\n","        },        \n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep_config, project='CS6910_Assignment2')\n","# wandb.agent(sweep_id, train, count=1)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Filter Visualization"]},{"cell_type":"markdown","metadata":{},"source":["### Load best model for visualization"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:35:04.756509Z","iopub.status.busy":"2022-04-11T17:35:04.756240Z","iopub.status.idle":"2022-04-11T17:35:09.273137Z","shell.execute_reply":"2022-04-11T17:35:09.272345Z","shell.execute_reply.started":"2022-04-11T17:35:04.756478Z"},"trusted":true},"outputs":[],"source":["best_run_path = 'ms20/CS6910_Assignment2/runs/odlwk86t/'\n","\n","api = wandb.Api()\n","run = api.run(best_run_path)\n","\n","model_file = run.file('model-best.h5').download(replace=True)\n","model = tf.keras.models.load_model(model_file.name)\n","config_file = run.file('config.yaml').download(replace=True)\n","\n","with open(config_file.name, 'r') as file:\n","    config = yaml.safe_load(file)"]},{"cell_type":"markdown","metadata":{},"source":["### Load Random Images"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:35:09.275075Z","iopub.status.busy":"2022-04-11T17:35:09.274808Z","iopub.status.idle":"2022-04-11T17:35:10.714624Z","shell.execute_reply":"2022-04-11T17:35:10.713883Z","shell.execute_reply.started":"2022-04-11T17:35:09.275036Z"},"trusted":true},"outputs":[],"source":["# Load n samples from the test set for analysis\n","n = 30\n","\n","files = glob.glob('./inaturalist_12K/val/*/*')\n","# select n random files\n","random_image_samples = random.sample(files, n)\n","# Convert them to np arrays\n","random_images = []\n","actual_labels = []\n","predicted_labels = []\n","for image in random_image_samples:\n","    img = Image.open(image)\n","    img = img.resize(config[\"img_size\"][\"value\"])\n","    img = np.array(img)\n","    random_images.append(img)\n","    actual_labels.append(image.split('/')[-2])\n","\n","    \n","predicted_class_number = np.argmax(model.predict(np.array(random_images)), axis=-1)\n","predicted_labels = [class_labels[class_no]\n","                    for class_no in predicted_class_number]\n"]},{"cell_type":"markdown","metadata":{},"source":["### Plot the images with prediction results"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:35:10.716365Z","iopub.status.busy":"2022-04-11T17:35:10.716121Z","iopub.status.idle":"2022-04-11T17:35:13.339100Z","shell.execute_reply":"2022-04-11T17:35:13.338408Z","shell.execute_reply.started":"2022-04-11T17:35:10.716330Z"},"trusted":true},"outputs":[],"source":["row, col = 10, 3\n","fig, ax = plt.subplots(row, col, figsize=(15, 18))\n","fig.suptitle(\"Model Evaluation on random images\", fontsize='x-large')\n","for i in range(row):\n","    for j in range(col):\n","        idx = i*col + j\n","        ax[i][j].imshow(Image.open(\n","            random_image_samples[idx]).resize((500, 500)))\n","        ax[i][j].axis('off')\n","        ax[i][j].set_title(f'True: {actual_labels[idx]}')\n","\n","        if actual_labels[idx] == predicted_labels[idx]:\n","            text_color = \"green\"\n","        else:\n","            text_color = \"red\"\n","\n","        ax[i][j].text(0., -0.5, \"Pred : \", color='black', transform=ax[i][j].transAxes)\n","        ax[i][j].text(0.7, -0.5, predicted_labels[idx], color=text_color, transform=ax[i][j].transAxes)\n","\n","fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T17:35:22.818759Z","iopub.status.busy":"2022-04-11T17:35:22.818493Z","iopub.status.idle":"2022-04-11T17:35:38.332313Z","shell.execute_reply":"2022-04-11T17:35:38.331563Z","shell.execute_reply.started":"2022-04-11T17:35:22.818727Z"},"trusted":true},"outputs":[],"source":["# Log the image \n","wandb.init()\n","wandb.log({\"plot\": fig})\n","wandb.finish()"]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2022-04-11T17:40:29.967319Z","iopub.status.busy":"2022-04-11T17:40:29.966666Z","iopub.status.idle":"2022-04-11T17:40:54.843936Z","shell.execute_reply":"2022-04-11T17:40:54.842904Z","shell.execute_reply.started":"2022-04-11T17:40:29.967279Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["# Filter Visualization\n","random_image_path = random.sample(glob.glob('./inaturalist_12K/val/*/*'), 1)[0]\n","rand_image = np.array(Image.open(random_image_path).resize(config['img_size']['value']))\n","\n","# The first convolution layer of the model is named 'conv2d'\n","layer = model.get_layer(name='conv2d')\n","# Create an intermediate model to tap the output of the first layer\n","intermediate_model = tf.keras.Model(inputs=model.input, outputs=layer.output)\n","# Get the output of the first convolution layer\n","layer_outputs = intermediate_model.predict(rand_image.reshape(1, *rand_image.shape))\n","# Getting layer weights of the first convolution layer\n","layer_weights = layer.weights\n","\n","# Plot the sample image\n","fig = plt.figure()\n","plt.title('Image used for visualizing filters', fontsize='x-large')\n","plt.axis('off')\n","plt.imshow(np.array(Image.open(random_image_path)))\n","\n","# Plot the filters and layer weights\n","row, col = int(config['number_of_filters_base']['value'] / 8), 8\n","fig1, ax1 = plt.subplots(row, col, figsize=(25, 50), )\n","fig2, ax2 = plt.subplots(row, col, figsize=(25, 50))\n","# Set title of the figure\n","fig1.suptitle(f\"Visualizing output of {config['number_of_filters_base']['value']} filters in first layer\", fontsize='x-large')\n","fig2.suptitle(f\"Visualizing {config['number_of_filters_base']['value']} filters in first layer\", fontsize='x-large')\n","\n","for i in range(row):\n","    for j in range(col):\n","        idx = i*col + j\n","        ax1[i][j].set_title(f'Filter : {idx+1}')\n","        ax1[i][j].axis('off')\n","        ax1[i][j].imshow(layer_outputs[0][:, :, idx], cmap='plasma')\n","\n","        ax2[i][j].set_title(f'Filter : {idx+1}')\n","        ax2[i][j].axis('off')\n","        ax2[i][j].imshow(layer_weights[0].numpy()[:, :, 0, idx], cmap=\"gray\")\n","\n","fig1.tight_layout(rect=[0, 0.03, 0.5, 0.95])\n","fig2.tight_layout(rect=[0, 0.03, 0.5, 0.95])\n","\n","fig1.show()\n","fig2.show()\n","\n","wandb.init()\n","wandb.log({'random_image': wandb.Image(Image.open(random_image_path))})\n","wandb.log({'filters': fig2})\n","wandb.log({'filter_outputs': fig1})\n","run.finish()"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T18:08:37.121468Z","iopub.status.busy":"2022-04-11T18:08:37.121134Z","iopub.status.idle":"2022-04-11T18:08:37.127976Z","shell.execute_reply":"2022-04-11T18:08:37.127051Z","shell.execute_reply.started":"2022-04-11T18:08:37.121435Z"},"trusted":true},"outputs":[],"source":["@tf.custom_gradient\n","def guidedRelu(x):\n","    def grad(dy):\n","        return tf.cast(dy>0,\"float32\") * tf.cast(x>0, \"float32\") * dy\n","    return tf.nn.relu(x), grad"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2022-04-11T18:33:13.007479Z","iopub.status.busy":"2022-04-11T18:33:13.006981Z","iopub.status.idle":"2022-04-11T18:33:29.377437Z","shell.execute_reply":"2022-04-11T18:33:29.376674Z","shell.execute_reply.started":"2022-04-11T18:33:13.007436Z"},"trusted":true},"outputs":[],"source":["def guided_backprop(config, model):\n","    # Function to perform guided backpropagation on the model\n","    random_image_path = random.sample(\n","        glob.glob('./inaturalist_12K/val/*/*'), 1)[0]\n","    rand_inp_image = np.array(Image.open(\n","        random_image_path).resize(config['img_size']['value']))\n","\n","    # Plot the image used for guided backprop\n","    plt.title('Random Image used for guided backpropogation', fontsize='x-large')\n","    plt.axis('off')\n","    plt.imshow(np.array(Image.open(random_image_path).resize(\n","        config['img_size']['value'])))\n","    plt.show()\n","\n","    gb_model = tf.keras.Model(\n","        inputs=[model.inputs],\n","        outputs=[model.get_layer(\"conv2d_4\").output]\n","    )\n","\n","    output_shape = model.get_layer(\"conv2d_4\").output.shape[1:]\n","    layer_dict = [layer for layer in gb_model.layers if hasattr(\n","        layer, 'activation')]\n","\n","    for layer in layer_dict:\n","        if layer.activation == tf.keras.activations.relu:\n","            layer.activation = guidedRelu\n","\n","    row, col = 5, 2\n","    fig, ax = plt.subplots(row, col, figsize=(15, 28))\n","\n","    fig.suptitle(\n","        \"Visualizing gradient for 10 neurons in CONV-5 layer\", fontsize='x-large')\n","\n","    for i in range(10):\n","        rand_neuron_index = [\n","            0] + [random.randint(0, dim_max-1) for dim_max in output_shape]\n","\n","        mask = np.zeros((1, *output_shape))\n","        mask[rand_neuron_index[0], rand_neuron_index[1],\n","             rand_neuron_index[2], rand_neuron_index[3]] = 1\n","\n","        with tf.GradientTape() as tape:\n","            inputs = tf.cast(rand_inp_image.reshape(\n","                1, *rand_inp_image.shape), tf.float32)\n","            tape.watch(inputs)\n","            outputs = (gb_model(inputs) * mask)\n","\n","        grad = tape.gradient(outputs, inputs)[0]\n","        grad -= np.mean(grad)\n","        grad /= (np.std(grad) + 1e-5)\n","        ax[i//col, i % col].set_title(f'Neuron-{i+1}')\n","        ax[i//col, i % col].axis('off')\n","        ax[i//col, i % col].imshow(grad)\n","\n","    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n","    fig.show()\n","\n","    # Visulizing the guided back propagation for the whole image\n","    with tf.GradientTape() as tape:\n","        inputs = tf.cast(rand_inp_image.reshape(\n","            1, *rand_inp_image.shape), tf.float32)\n","        tape.watch(inputs)\n","        outputs = gb_model(inputs)\n","\n","    grad = tape.gradient(outputs, inputs)[0]\n","\n","    fig_whole = plt.figure()\n","    plt.title('Visualizing gradient for whole CONV-5 layer', fontsize='x-large')\n","    plt.axis('off')\n","    plt.imshow(np.array(grad))\n","    plt.show()\n","\n","    # Gradcam\n","#     with tf.GradientTape() as tape:\n","#         inputs = tf.cast(rand_inp_image.reshape(\n","#             1, *rand_inp_image.shape), tf.float32)\n","#         tape.watch(inputs)\n","#         outputs = gb_model(inputs)[0]\n","\n","#     grad = tape.gradient(outputs, inputs)[0]\n","\n","#     weights = tf.reduce_mean(grad, axis=(0, 1))\n","#     grad_cam = np.ones(outputs.shape[0: 2], dtype=np.float32)\n","\n","#     for j, w in enumerate(weights):\n","#         grad_cam += w * outputs[:, :, j]\n","\n","#     grad_cam_img = cv2.resize(grad_cam.numpy(), (224, 224))\n","#     grad_cam_img = np.maximum(grad_cam_img, 0)\n","#     heatmap = (grad_cam_img - grad_cam_img.min()) / (grad_cam_img.max() - grad_cam_img.min())\n","#     grad_cam_img = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n","#     output_image = cv2.addWeighted(cv2.cvtColor(rand_inp_image.astype('uint8'), cv2.COLOR_RGB2BGR), 0.5, grad_cam_img, 1, 0)\n","\n","#     fig_grad_cam = plt.figure()\n","#     plt.title('Visualizing gradient for whole CONV-5 layer', fontsize='x-large')\n","#     plt.axis('off')\n","#     plt.imshow(np.array(output_image))\n","#     plt.show()\n","\n","    return fig, fig_whole, random_image_path\n","\n","\n","wandb.init()\n","fig, fig_whole, random_image_path = guided_backprop(config, model)\n","wandb.log({'random_image': wandb.Image(Image.open(random_image_path))})\n","wandb.log({'guided_backprop_10_neurons_visualization': fig})\n","wandb.log({'guided_backprop_whole_visualization': fig_whole})\n","# wandb.log({'Gradcam_visualization': fig_gradcam})\n","wandb.finish()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
